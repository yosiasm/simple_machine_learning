{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengantar Neural Network\n",
    "\n",
    "![Meme2](./images/meme2.jpg)\n",
    "\n",
    "Sumber: https://www.meme-arsenal.com/en/create/meme/410387\n",
    "\n",
    "Neural Network adalah salah satu model machine learning yang meniru cara kerja neuron pada otak manusia, didesain untuk mengenali pola. \n",
    "\n",
    "Mengapa perlu mempelajari metode yang terinspirasi dari alam atau makhluk hidup?\n",
    "\n",
    "- Karena makhluk hidup/alam adalah salah satu *problem-solver* terbaik.\n",
    "- Proses evolusi\n",
    "- Kecerdasan alamiah dan beberapa penerapannya seperti: \n",
    "    - Celular automata\n",
    "    - Komputasi evolusioner\n",
    "    - *Particle Swarm Optimization*\n",
    "    - Artificial Immune Systems\n",
    "    - Artificial Neural Network\n",
    "\n",
    "![Neuron](./images/neuron.jpg)\n",
    "\n",
    "Sumber: https://simple.wikipedia.org/wiki/Neuron\n",
    "\n",
    "Cara kerja neuron adalah sebagai berikut:\n",
    "\n",
    "- **Dendrit** berfungsi sebagai penerima reseptor, dimana sel dapat menerima signal berukuran sangat besar dari neuron sekitarnya. \n",
    "- **Soma** berfungsi untuk mengumpulkan/mengagregasi semua signal yang diterima dari dendrit.\n",
    "- **Axon** meneruskan signal hasil dari agregasi yang dilakukan soma. Signal ini memiliki semacam potensial listrik yang ketika mencapai ukuran tertentu, axon akan meneruskan signal ini melaluinya. Axon juga bertindak sebagai penghubung antar neuron lainnya.\n",
    "\n",
    "## Struktur Neural Network\n",
    "\n",
    "Struktur paling sederhana dari neural network adalah perceptron seperti pada gambar dibawah ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](./images/perceptron.png)\n",
    "\n",
    "Sumber: https://www.researchgate.net/figure/Single-Layer-Perceptron-Network-Multilayer-Perceptron-Network-These-type-of-feed-forward_fig2_283339701\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secara sederhana, neural network bekerja berdasarkan beberapa bagian, diantaranya:\n",
    "\n",
    "- *input*, bagian dimana kita dapat memasukkan beberapa variabel yang dipakai untuk pemodelan.\n",
    "- *weight* atau bobot, bagian yang mengatur bagaimana tiap input dihitung\n",
    "- *activation function*, bagian dimana tiap bobot tadi dihitung sehingga menghasilkan *output* yang diinginkan\n",
    "\n",
    "Dalam bahasa matematika formal:\n",
    "\n",
    "Input $\\vec{x} \\in \\mathbb{R}$ dikali dengan nilai bobot $\\vec{w}$ ditambah dengan nilai bias $b$ untuk menghasilkan nilai agregasi $z$. Nilai $z$ ini akan dibawa ke fungsi aktivasi $f(z)$ untuk memperoleh nilai output $\\hat{y}$.\n",
    "\n",
    "$$\\hat{y} = f(z) = f(\\vec{w} \\bullet \\vec{x} + b) = f(\\sum_{i=1}^{n} w_{i} x_{i} + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Aktivasi\n",
    "\n",
    "Fungsi aktivasi memberikan nilai output dari suatu neuron berdasarkan kombinasi dari nilai tiap input dan *weight*. Beberapa fungsi aktivasi yang biasa digunakan diantaranya adalah:\n",
    "\n",
    "**Sigmoid Function**\n",
    "\n",
    "![Sigmoid](./images/Activation_logistic.svg.png)\n",
    "\n",
    "Sumber: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "Sigmoid function secara matematis didefinisikan sebagai berikut:\n",
    "\n",
    "$$f(z) = \\frac{1}{1+\\exp{(-z)}}$$\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Menghindari nilai output yang melompat secara ekstrim\n",
    "- Output berada di antara 0 dan 1\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Vanishing gradient, untuk nilai-nilai ekstrim, tidak ada perubahan signifikan pada hasil prediksi. Hal ini dapat menyebabkan model terlalu lama dalam membuat hasil prediksi\n",
    "- Hanya untuk nilai biner\n",
    "- Computational cost sangat besar\n",
    "\n",
    "**Tanh Function**\n",
    "\n",
    "![Tanh](./images/640px-Activation_tanh.svg.png)\n",
    "\n",
    "Sumber: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "Tanh function secara matematis didefinisikan sebagai berikut:\n",
    "\n",
    "$$f(z) = \\tanh(z) = \\frac{e^{z}-e^{-z}}{e^{z} + e^{-z}}$$\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Dapat digunakan untuk klasifikasi non-biner\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Mirip dengan sigmoid function\n",
    "\n",
    "**Rectified Linear Unit (ReLU)**\n",
    "\n",
    "![ReLU](./images/640px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "Sumber: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "ReLU secara matematis didefinisikan sebagai berikut:\n",
    "\n",
    "$$\n",
    "f(z)=\n",
    "\\begin{cases}\n",
    "0, \\text{jika } x \\leq 0\\\\\n",
    "z, \\text{jika } x \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Computational cost lebih rendah\n",
    "- Non-linear\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Jika input bernilai negatif atau 0, maka neural network tidak dapat melakukan proses learning. Hal ini disebut sebagai *dying ReLU problem*.\n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "![Leaky](./images/640px-Activation_prelu.svg.png)\n",
    "\n",
    "Sumber: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "Leaky ReLU merupakan pengembangan yang dibuat untuk mengatasi *dying ReLU problem*. Secara matematis dirumuskan sebagai berikut:\n",
    "\n",
    "$$\n",
    "f(z)=\n",
    "\\begin{cases}\n",
    "0.01z, \\text{jika } x \\lt 0\\\\\n",
    "z, \\text{jika } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Mirip seperti ReLU, namun tanpa *dying ReLU problem* \n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Hasil dari leaky ReLU tidak konsisten untuk input bernilai negatif\n",
    "\n",
    "**Parametric ReLU**\n",
    "\n",
    "Parametric ReLU pengembangan selanjutnya dari leaky ReLU. Secara matematis didefinisikan sebagai berikut:\n",
    "\n",
    "$$\n",
    "f(\\alpha, z)=\n",
    "\\begin{cases}\n",
    "\\alpha z, \\text{jika } x \\lt 0\\\\\n",
    "z, \\text{jika } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Tidak seperti leaky ReLU, fungsi ini dapat menerima input bernilai negatif\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Hasilnya tidak konsisten, tergantung dari masalah yang diselesaikan\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "Softmax secara matematis didefinisikan sebagai berikut:\n",
    "\n",
    "$$f(\\vec{z}) = \\frac{\\exp{z_{i}}}{\\Sigma^{J}_{i=j}{\\exp{z_{j}}}}$$\n",
    "\n",
    "Untuk $j = 1, ..., J$\n",
    "\n",
    "Kelebihan: \n",
    "\n",
    "- Dapat digunakan untuk output yang lebih dari 2 atau multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arsitektur Neural Network\n",
    "\n",
    "\n",
    "Berikut merupakan beberapa arsitektur dari model neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron**\n",
    "\n",
    "Perceptron adalah bentuk paling sederhana dari neural network, hanya terdiri dari penerima input dan fungsi aktivasi. Perceptron dapat dikatakan sebagai *building block* dari neural network.\n",
    "\n",
    "![Perceptron](./images/perceptron.png)\n",
    "\n",
    "Sumber: Sumber: https://www.researchgate.net/figure/Single-Layer-Perceptron-Network-Multilayer-Perceptron-Network-These-type-of-feed-forward_fig2_283339701\n",
    "\n",
    "Secara matematis, perceptron bekerja sebagai berikut:\n",
    "\n",
    "$\\vec{x}$ dan $\\vec{w}$ dipakai untuk mencari nilai dari $\\hat{y}$:\n",
    "\n",
    "$$\\hat{y} = f(\\vec{w} \\bullet \\vec{x} + b) = f(w_{1} x_{1} + ... + w_{n} x_{n} + b)$$\n",
    "\n",
    "Dimana nilai $\\hat{y} = f(\\vec{x})$ didefinisikan sebagai berikut:\n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\begin{cases}\n",
    "        1, & \\text{jika } \\vec{w} \\bullet \\vec{x} + b > 0 \\\\\n",
    "        0, & \\text{lainnya }\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Selanjutnya nilai bias dan bobot diperbaharui dengan cara seperti berikut:\n",
    "\n",
    "$$w_{i} = w_{i} + \\alpha(y - \\hat{y}x_{i}), i=1,...,n$$\n",
    "\n",
    "$$b = b+\\alpha(y-\\hat{y})$$\n",
    "\n",
    "dengan $\\alpha \\in (0,1]$ disebut sebagai *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Layered Perceptron**\n",
    "\n",
    "Multi-layered perceptron, atau yang biasa disingkat sebagai MLP, adalah gabungan dari beberapa perceptron sehingga membentuk beberapa lapisan (*layer*). Biasanya MLP terdiri dari 3 lapis, yaitu input layer, hidden layer dan output layer.\n",
    "\n",
    "![MLP](./images/Typical-structure-of-a-feed-forward-multilayer-neural-network.png)\n",
    "\n",
    "Sumber: https://www.researchgate.net/figure/Typical-structure-of-a-feed-forward-multilayer-neural-network_fig1_291339457\n",
    "\n",
    "Secara matematis MLP dirumuskan sebagai berikut:\n",
    "\n",
    "$$y^{l}_{j} = f^{l}(z^{l}_{j})$$\n",
    "\n",
    "dimana:\n",
    "\n",
    "- $y^{l}_{j}$ adalah output ke-$j$ pada layer ke-$l$\n",
    "- $x^{l}_{i}$ adalah input ke-$i$ pada layer ke-$l$\n",
    "- $w^{l}_{ji}$ adalah bobot untuk neuron ke-$j$ yang terkoneksi dengan input ke-$i$ pada layer ke-$l$\n",
    "- $z^{l}_{j}$ adalah hasil dot product antara $x^{l}_{j}$ dengan $w^{l}_{ji}$\n",
    "- $f^{l}(z)$ adalah fungsi aktivasi pada layer ke-$l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurent Neural Network (RNN)**\n",
    "\n",
    "Recurent neural network adalah neural network yang mempunyai *feedback loop*.\n",
    "\n",
    "![RNN](./images/rnn.png)\n",
    "\n",
    "Sumber: https://austingwalters.com/classify-sentences-via-a-recurrent-neural-network-lstm/\n",
    "\n",
    "Terdapat beberapa arsitektur turunan dari RNN, diantaranya:\n",
    "\n",
    "- Long-Short Term Memory (LSTM)\n",
    "\n",
    "- Gated Recurent Unit (GRU)\n",
    "\n",
    "![LSTM - GRU](./images/lstm-gru.png)\n",
    "\n",
    "Sumber: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network**\n",
    "\n",
    "Convolutional neural network pada dasarnya sama dengan MLP, hanya saja memiliki layer lain yang digunakan sebagai ekstraksi fitur pada data (biasanya gambar).\n",
    "\n",
    "![CNN](./images/cnn.jpeg)\n",
    "\n",
    "Sumber: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "![Contoh CNN](./images/Architecture-of-the-Residual-Network-for-Object-Photo-Classification.png)\n",
    "\n",
    "Sumber: https://machinelearningmastery.com/review-of-architectural-innovations-for-convolutional-neural-networks-for-image-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembahasan tentang CNN dan RNN serta penerapannya akan dibahas di bab khusus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks\n",
    "\n",
    "![DL Frameworks](./images/dlframework.jpg)\n",
    "\n",
    "Sumber: https://sisyphus.gitbook.io/project/deep-learning-basics/deep-learning-frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
